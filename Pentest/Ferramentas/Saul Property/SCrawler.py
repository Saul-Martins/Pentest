from urllib.request import urljoin
from bs4 import BeautifulSoup
import requests
from urllib.parse import urlparse
import warnings

# Ignorar os avisos de depreciação e SSL
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", message="Unverified HTTPS request")

links_intern = set()
links_extern = set()

input_url = input('DIGITE A URL: ')
nome_arquivo = input('DIGITE O NOME DO ARQUIVO TXT: ')
depth = int(input('Digite a profundidade desejada (por exemplo, 1 ou 2): '))

def level_crawler(input_url):
    temp_urls = set()
    current_url_domain = urlparse(input_url).netloc

    try:
        # Suprimir avisos SSL e fazer requisição
        response = requests.get(input_url, verify=False)
        beautiful_soup_object = BeautifulSoup(response.content, "lxml")

        for anchor in beautiful_soup_object.findAll("a"):
            href = anchor.attrs.get("href")
            if href != "" and href is not None:
                href = urljoin(input_url, href)
                href_parsed = urlparse(href)
                href = f"{href_parsed.scheme}://{href_parsed.netloc}{href_parsed.path}"
                final_parsed_href = urlparse(href)
                is_valid = bool(final_parsed_href.scheme) and bool(final_parsed_href.netloc)

                if is_valid:
                    if current_url_domain not in href and href not in links_extern:
                        print(f"Externo - {href}")
                        links_extern.add(href)
                    if current_url_domain in href and href not in links_intern:
                        print(f"Interno - {href}")
                        links_intern.add(href)
                        temp_urls.add(href)
    except Exception as e:
        print(f"Erro ao processar {input_url}: {e}")

    return temp_urls

if depth == 0:
    print(f"Interno - {input_url}")

elif depth == 1:
    try:
        level_crawler(input_url)
    except:
        pass

else:
    queue = [input_url]
    with open(nome_arquivo, 'w') as fp:
        for j in range(depth):
            for count in range(len(queue)):
                url = queue.pop(0)
                urls = level_crawler(url)
                for i in urls:
                    queue.append(i)
                    fp.write(i + '\n')
                    fp.flush()